{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":14340760,"sourceType":"datasetVersion","datasetId":9156311}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm\nfrom pytorch_lightning import LightningModule\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\n\nfrom types import SimpleNamespace","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:06.448298Z","iopub.execute_input":"2025-12-30T10:12:06.448962Z","iopub.status.idle":"2025-12-30T10:12:06.455005Z","shell.execute_reply.started":"2025-12-30T10:12:06.448936Z","shell.execute_reply":"2025-12-30T10:12:06.454209Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"DATA_ROOT = '/kaggle/input/csiro-biomass/'\n\n# train\ntrain_df = pd.read_csv(f'{DATA_ROOT}/train.csv')\ntrain_df[['sample_id_prefix', 'sample_id_suffix']] = train_df.sample_id.str.split('__', expand=True)\n\n# agg_train_df の作成\ncols = ['sample_id_prefix', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']\nagg_train_df = train_df.groupby(cols).apply(lambda df: df.set_index('target_name').target)\nagg_train_df.reset_index(inplace=True)\nagg_train_df.columns.name = None\n\nagg_train_df['image'] = agg_train_df.image_path.progress_apply(\n    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:07.003558Z","iopub.execute_input":"2025-12-30T10:12:07.003829Z","iopub.status.idle":"2025-12-30T10:12:23.363658Z","shell.execute_reply.started":"2025-12-30T10:12:07.003811Z","shell.execute_reply":"2025-12-30T10:12:23.362907Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2742864213.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  agg_train_df = train_df.groupby(cols).apply(lambda df: df.set_index('target_name').target)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e95951f8444ca395076099ed06cb9c"}},"metadata":{}}],"execution_count":114},{"cell_type":"code","source":"# 画像サイズ確認\nagg_train_df['image_size'] = agg_train_df.image.apply(lambda x: x.size)\nagg_train_df['image_size'].value_counts()\n\n# ターゲット合計確認\nnp.isclose(agg_train_df[['Dry_Green_g', 'Dry_Clover_g']].sum(axis=1),\n           agg_train_df['GDM_g'], atol=1e-4).mean()\n\nnp.isclose(agg_train_df[['GDM_g', 'Dry_Dead_g']].sum(axis=1),\n           agg_train_df['Dry_Total_g'], atol=1e-4).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.364779Z","iopub.execute_input":"2025-12-30T10:12:23.364999Z","iopub.status.idle":"2025-12-30T10:12:23.375851Z","shell.execute_reply.started":"2025-12-30T10:12:23.364981Z","shell.execute_reply":"2025-12-30T10:12:23.375154Z"}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"0.9971988795518207"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"# test.csv\ntest_df = pd.read_csv(DATA_ROOT + 'test.csv')\ntest_df[['sample_id_prefix', 'sample_id_suffix']] = test_df.sample_id.str.split('__', expand=True)\n\n# 推論用 agg_test_df\nagg_test_df = test_df.drop_duplicates(subset='sample_id_prefix').copy()\n\nagg_test_df['image'] = agg_test_df.image_path.progress_apply(\n    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.376708Z","iopub.execute_input":"2025-12-30T10:12:23.376968Z","iopub.status.idle":"2025-12-30T10:12:23.455765Z","shell.execute_reply.started":"2025-12-30T10:12:23.376950Z","shell.execute_reply":"2025-12-30T10:12:23.455040Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dd595059f324c32bf98cce42beb345d"}},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"class InferenceDataset(Dataset):\n    def __init__(self, df, transforms):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = self.df.iloc[idx][\"image\"]\n        if self.transforms:\n            image = self.transforms(image=np.array(image))[\"image\"]\n        return image\n\ndef get_val_transforms(cfg):\n    return A.Compose([\n        A.Resize(height=cfg.task.img_size, width=cfg.task.img_size, p=1.0),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(p=1.0)\n    ])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.457842Z","iopub.execute_input":"2025-12-30T10:12:23.458141Z","iopub.status.idle":"2025-12-30T10:12:23.463861Z","shell.execute_reply.started":"2025-12-30T10:12:23.458119Z","shell.execute_reply":"2025-12-30T10:12:23.463137Z"}},"outputs":[],"execution_count":117},{"cell_type":"code","source":"class TimmEncoder(nn.Module):\n    def __init__(self, cfg, **kwargs):\n        super().__init__()\n\n        self.encoder = timm.create_model(\n            cfg.model.backbone,\n            in_chans=3,\n            pretrained=False,        # ← 推論では必ず False\n            features_only=False,\n            num_classes=0,\n        )\n\n        self.out_feature_dim = self.encoder.num_features\n        self.classifier = nn.Linear(self.out_feature_dim, 3)\n\n    def forward(self, x: torch.Tensor):\n        features = self.encoder.forward_features(x)\n\n        if isinstance(features, (list, tuple)):\n            features = features[-1]\n\n        if features.dim() == 4:\n            features = features.mean(dim=(2, 3))\n\n        return self.classifier(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.464525Z","iopub.execute_input":"2025-12-30T10:12:23.464743Z","iopub.status.idle":"2025-12-30T10:12:23.480412Z","shell.execute_reply.started":"2025-12-30T10:12:23.464723Z","shell.execute_reply":"2025-12-30T10:12:23.479627Z"}},"outputs":[],"execution_count":118},{"cell_type":"code","source":"def get_model_from_cfg(cfg):\n    if cfg.model.arch == \"timm_encoder\":\n        model = TimmEncoder(cfg)\n    else:\n        raise ValueError(f\"Unknown model architecture: {cfg.model.arch}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.481188Z","iopub.execute_input":"2025-12-30T10:12:23.481417Z","iopub.status.idle":"2025-12-30T10:12:23.492807Z","shell.execute_reply.started":"2025-12-30T10:12:23.481393Z","shell.execute_reply":"2025-12-30T10:12:23.491922Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"\ndef get_loss(cfg):\n    return MyLoss(cfg)\n\nclass MyLoss(nn.Module):\n    def __init__(self, cfg):\n        super(MyLoss, self).__init__()\n        self.cfg = cfg\n\n        # 基本は SmoothL1（元コードと同じ）\n        self.criterion = nn.SmoothL1Loss(reduction=\"mean\")\n\n        # 将来の拡張用（今は使わないが cfg で制御できる）\n        self.use_gdm = getattr(cfg.loss, \"use_gdm\", False)\n        self.use_total = getattr(cfg.loss, \"use_total\", False)\n\n    def forward(self, y_pred, y_true):\n        \"\"\"\n        Args:\n            y_pred (Tensor[float]): (batch_size, 3)\n                [Dry_Green_g, Dry_Clover_g, Dry_Dead_g]\n            y_true (Tensor[float]): (batch_size, 3)\n\n        Returns:\n            dict:\n                {\n                    \"loss\": total_loss,\n                    \"loss_reg\": regression_loss,\n                }\n        \"\"\"\n        return_dict = {}\n\n        # 形状チェック\n        assert y_pred.shape == y_true.shape, \\\n            f\"y_pred: {y_pred.shape}, y_true: {y_true.shape}\"\n\n        # --- 基本回帰損失 ---\n        loss_reg = self.criterion(y_pred, y_true)\n        loss_total = loss_reg\n\n        # --- 拡張例：GDM / Total を loss に含めたい場合 ---\n        if self.use_gdm:\n            # GDM = Green + Clover\n            gdm_pred = y_pred[:, 0] + y_pred[:, 1]\n            gdm_true = y_true[:, 0] + y_true[:, 1]\n            loss_gdm = self.criterion(gdm_pred, gdm_true)\n            loss_total = loss_total + loss_gdm\n            return_dict[\"loss_gdm\"] = loss_gdm\n\n        if self.use_total:\n            # Total = Green + Clover + Dead\n            total_pred = y_pred.sum(dim=1)\n            total_true = y_true.sum(dim=1)\n            loss_total_biomass = self.criterion(total_pred, total_true)\n            loss_total = loss_total + loss_total_biomass\n            return_dict[\"loss_total_biomass\"] = loss_total_biomass\n\n        return_dict[\"loss_reg\"] = loss_reg\n        return_dict[\"loss\"] = loss_total\n\n        return return_dict\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.493681Z","iopub.execute_input":"2025-12-30T10:12:23.493946Z","iopub.status.idle":"2025-12-30T10:12:23.506579Z","shell.execute_reply.started":"2025-12-30T10:12:23.493925Z","shell.execute_reply":"2025-12-30T10:12:23.505904Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"\nclass MyModel(LightningModule):\n    def __init__(self, cfg, mode=\"train\"):\n        super().__init__()\n        self.preds = None\n        self.gts = None\n\n        self.cfg = cfg\n        self.mode = mode\n        \n        self.model = get_model_from_cfg(cfg)\n\n        # epoch 集計用\n        self.val_outputs = []\n        self.val_targets = []\n\n        if mode != \"test\" and cfg.model.ema:\n            self.model_ema = ModelEmaV3(\n                self.model,\n                decay=cfg.model.ema_decay,\n                update_after_step=cfg.model.ema_update_after_step,\n            )\n\n        self.loss = get_loss(cfg)\n\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        images, targets = batch  # (B, 3)\n        targets = targets.float()\n\n        outputs = self(images)\n        loss_dict = self.loss(outputs, targets)\n\n        self.log_dict(\n            loss_dict,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        return loss_dict[\"loss\"]\n\n    def on_train_batch_end(self, out, batch, batch_idx):\n        if self.cfg.model.ema:\n            self.model_ema.update(self.model)\n\n    def validation_step(self, batch, batch_idx):\n        images, targets = batch\n        targets = targets.float()\n\n        outputs = self(images)\n        loss_dict = self.loss(outputs, targets)\n\n        self.log(\"val_loss\", loss_dict[\"loss\"], prog_bar=True, sync_dist=True)\n        \n        self.val_outputs.append(outputs.detach())\n        self.val_targets.append(targets.detach())\n\n        return loss_dict\n\n    def on_validation_epoch_end(self):\n        outputs = torch.cat(self.val_outputs).cpu().numpy()\n        targets = torch.cat(self.val_targets).cpu().numpy()\n\n        weighted_r2, r2_scores = calc_metric(outputs, targets)\n\n        # メトリクスをログ\n        self.log(\"val_weighted_r2\", weighted_r2, prog_bar=True)\n\n        # 複数ターゲットなら個別ログも可\n        for i, r2 in enumerate(r2_scores):\n            self.log(f\"val_r2_target_{i}\", r2)\n\n        # 次epochに向けてクリア\n        self.val_outputs.clear()\n        self.val_targets.clear()\n\n        # 複数ターゲットなら個別ログも可\n        for i, r2 in enumerate(r2_scores):\n            self.log(f\"val_r2_target_{i}\", r2)\n\n        # 次epochに向けてクリア\n        self.val_outputs.clear()\n        self.val_targets.clear()\n\n    def configure_optimizers(self):\n        optimizer = create_optimizer_v2(model_or_params=self.model, **self.cfg.opt)\n\n        scheduler, _ = create_scheduler_v2(\n            optimizer=optimizer,\n            num_epochs=self.cfg.trainer.max_epochs,\n            **self.cfg.scheduler\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"monitor\": \"val_weighted_r2\",\n            },\n        }\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.current_epoch)\n    #     # scheduler.step_update(num_updates=self.global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.507309Z","iopub.execute_input":"2025-12-30T10:12:23.507592Z","iopub.status.idle":"2025-12-30T10:12:23.524884Z","shell.execute_reply.started":"2025-12-30T10:12:23.507573Z","shell.execute_reply":"2025-12-30T10:12:23.524207Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"from types import SimpleNamespace\n\ncfg = SimpleNamespace()\n\n# --- task ---\ncfg.task = SimpleNamespace(\n    img_size=224,\n    img_depth=16,\n    fixed_depth=16,\n    slice_depth=3,\n    pretrain=False,\n    dirname=\"train_npzs\"\n)\n\n# --- model ---\ncfg.model = SimpleNamespace(\n    freeze_end_epoch=0,\n    arch=\"timm_encoder\",\n    in_channels=16,\n    out_channels=1,\n    depth=4,\n    base_filters=64,\n    dropout=0.1,\n    use_batchnorm=True,\n    activation=\"relu\",\n    swa=False,\n    freeze_backbone=False,\n    backbone=\"efficientnet_b2\",\n    ema=False,\n    resume_path=\"byu0515_epoch=000_val_loss=0.0201.ckpt\",\n    drop_path_rate=0.0,\n    img_size=128,\n    img_depth=16,\n    kernel_size=5,\n    class_num=5\n)\n\n# --- data ---\ncfg.data = SimpleNamespace(\n    fold_num=5,\n    fold_id=0,\n    num_workers=8,\n    batch_size=32,\n    train_all=False,\n    input_dir=None,\n    output_dir=None,\n    val_output_dir=None\n)\n\n# --- trainer ---\ncfg.trainer = SimpleNamespace(\n    max_epochs=30,\n    devices=\"auto\",\n    strategy=\"auto\",\n    check_val_every_n_epoch=5,\n    sync_batchnorm=False,\n    accelerator=\"gpu\",\n    precision=32,\n    gradient_clip_val=None,\n    accumulate_grad_batches=1,\n    deterministic=True\n)\n\n# --- test ---\ncfg.test = SimpleNamespace(\n    mode=\"test\",\n    output_dir=\"preds_results\"\n)\n\n# --- opt ---\ncfg.opt = SimpleNamespace(\n    opt=\"AdamW\",\n    lr=1e-3,\n    weight_decay=0.01\n)\n\n# --- scheduler ---\ncfg.scheduler = SimpleNamespace(\n    sched=\"cosine\",\n    min_lr=0.0,\n    warmup_epochs=0\n)\n\n# --- loss ---\ncfg.loss = SimpleNamespace(\n    mixup=0.0,\n    cutmix=0.0\n)\n\n# --- wandb ---\ncfg.wandb = SimpleNamespace(\n    project=\"csiro2025\",\n    name=\"exp_0\",\n    fast_dev_run=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.525721Z","iopub.execute_input":"2025-12-30T10:12:23.525972Z","iopub.status.idle":"2025-12-30T10:12:23.540520Z","shell.execute_reply.started":"2025-12-30T10:12:23.525947Z","shell.execute_reply":"2025-12-30T10:12:23.539762Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nckpt_path = \"/kaggle/input/csiro-simple-exp1/exp_1_epoch019_val_loss8.7010.ckpt\"\n\nmodel = MyModel.load_from_checkpoint(ckpt_path, cfg=cfg, mode=\"test\")\nmodel.to(device)\nmodel.eval()\n\ntest_dataset = InferenceDataset(agg_test_df, get_val_transforms(cfg))\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n\ndef predict(model, dataloader, device):\n    model.to(device)\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for images in dataloader:\n            images = images.to(device)\n            outputs = model(images)\n            preds.append(outputs.cpu())\n    return torch.cat(preds).numpy()\n\npreds = predict(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:23.542652Z","iopub.execute_input":"2025-12-30T10:12:23.543015Z","iopub.status.idle":"2025-12-30T10:12:24.318118Z","shell.execute_reply.started":"2025-12-30T10:12:23.542997Z","shell.execute_reply":"2025-12-30T10:12:24.317288Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"agg_test_df[['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']] = preds\nagg_test_df['GDM_g'] = agg_test_df.Dry_Green_g + agg_test_df.Dry_Clover_g\nagg_test_df['Dry_Total_g'] = agg_test_df.GDM_g + agg_test_df.Dry_Dead_g\n\ncols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\nsub_df = agg_test_df.set_index('sample_id_prefix')[cols].stack().reset_index()\nsub_df.columns = ['sample_id_prefix', 'target_name', 'target']\nsub_df['sample_id'] = sub_df.sample_id_prefix + '__' + sub_df.target_name\n\nsub_df[['sample_id', 'target']].to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:24.319377Z","iopub.execute_input":"2025-12-30T10:12:24.319676Z","iopub.status.idle":"2025-12-30T10:12:24.335666Z","shell.execute_reply.started":"2025-12-30T10:12:24.319643Z","shell.execute_reply":"2025-12-30T10:12:24.335053Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"sub_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:24.336524Z","iopub.execute_input":"2025-12-30T10:12:24.336728Z","iopub.status.idle":"2025-12-30T10:12:24.353175Z","shell.execute_reply.started":"2025-12-30T10:12:24.336707Z","shell.execute_reply":"2025-12-30T10:12:24.352637Z"}},"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"  sample_id_prefix   target_name     target                   sample_id\n0     ID1001187975  Dry_Clover_g   1.142614  ID1001187975__Dry_Clover_g\n1     ID1001187975    Dry_Dead_g  11.051404    ID1001187975__Dry_Dead_g\n2     ID1001187975   Dry_Green_g  12.636003   ID1001187975__Dry_Green_g\n3     ID1001187975   Dry_Total_g  24.830021   ID1001187975__Dry_Total_g\n4     ID1001187975         GDM_g  13.778617         ID1001187975__GDM_g","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id_prefix</th>\n      <th>target_name</th>\n      <th>target</th>\n      <th>sample_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID1001187975</td>\n      <td>Dry_Clover_g</td>\n      <td>1.142614</td>\n      <td>ID1001187975__Dry_Clover_g</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID1001187975</td>\n      <td>Dry_Dead_g</td>\n      <td>11.051404</td>\n      <td>ID1001187975__Dry_Dead_g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID1001187975</td>\n      <td>Dry_Green_g</td>\n      <td>12.636003</td>\n      <td>ID1001187975__Dry_Green_g</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID1001187975</td>\n      <td>Dry_Total_g</td>\n      <td>24.830021</td>\n      <td>ID1001187975__Dry_Total_g</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID1001187975</td>\n      <td>GDM_g</td>\n      <td>13.778617</td>\n      <td>ID1001187975__GDM_g</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":125},{"cell_type":"code","source":"!head submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T10:12:24.353833Z","iopub.execute_input":"2025-12-30T10:12:24.354896Z","iopub.status.idle":"2025-12-30T10:12:24.674307Z","shell.execute_reply.started":"2025-12-30T10:12:24.354877Z","shell.execute_reply":"2025-12-30T10:12:24.673332Z"}},"outputs":[{"name":"stdout","text":"sample_id,target\nID1001187975__Dry_Clover_g,1.1426142\nID1001187975__Dry_Dead_g,11.051404\nID1001187975__Dry_Green_g,12.636003\nID1001187975__Dry_Total_g,24.83002\nID1001187975__GDM_g,13.778617\n","output_type":"stream"}],"execution_count":126}]}