{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":112509,"databundleVersionId":14254895,"sourceType":"competition"},{"sourceId":14353176,"sourceType":"datasetVersion","datasetId":9164919},{"sourceId":14392086,"sourceType":"datasetVersion","datasetId":9191576},{"sourceId":14537574,"sourceType":"datasetVersion","datasetId":9285102}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sys\n\nsys.path.insert(\n    0,\n    \"/kaggle/input/csiro-timm-latest/pytorch-image-models-1.0.22\"\n)\n\nimport timm\nprint(\"version:\", timm.__version__)\nprint(\"file:\", timm.__file__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:34:44.339517Z","iopub.execute_input":"2026-01-18T13:34:44.339822Z","iopub.status.idle":"2026-01-18T13:34:44.344890Z","shell.execute_reply.started":"2026-01-18T13:34:44.339799Z","shell.execute_reply":"2026-01-18T13:34:44.343937Z"}},"outputs":[{"name":"stdout","text":"version: 1.0.22\nfile: /kaggle/input/csiro-timm-latest/pytorch-image-models-1.0.22/timm/__init__.py\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport timm\nfrom pytorch_lightning import LightningModule\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom sklearn.model_selection import KFold, GroupKFold, StratifiedGroupKFold\n\nfrom types import SimpleNamespace","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:34:44.346036Z","iopub.execute_input":"2026-01-18T13:34:44.346283Z","iopub.status.idle":"2026-01-18T13:34:44.363556Z","shell.execute_reply.started":"2026-01-18T13:34:44.346255Z","shell.execute_reply":"2026-01-18T13:34:44.362790Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"timm.list_models(\"*dino*\")[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:34:44.364878Z","iopub.execute_input":"2026-01-18T13:34:44.365241Z","iopub.status.idle":"2026-01-18T13:34:44.378336Z","shell.execute_reply.started":"2026-01-18T13:34:44.365213Z","shell.execute_reply":"2026-01-18T13:34:44.377457Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"['vit_7b_patch16_dinov3',\n 'vit_base_patch14_dinov2',\n 'vit_base_patch14_reg4_dinov2',\n 'vit_base_patch16_dinov3',\n 'vit_base_patch16_dinov3_qkvb',\n 'vit_giant_patch14_dinov2',\n 'vit_giant_patch14_reg4_dinov2',\n 'vit_huge_plus_patch16_dinov3',\n 'vit_huge_plus_patch16_dinov3_qkvb',\n 'vit_large_patch14_dinov2']"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"DATA_ROOT = '/kaggle/input/csiro-biomass/'\n\n# train\ntrain_df = pd.read_csv(f'{DATA_ROOT}/train.csv')\ntrain_df[['sample_id_prefix', 'sample_id_suffix']] = train_df.sample_id.str.split('__', expand=True)\n\n# agg_train_df の作成\ncols = ['sample_id_prefix', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']\nagg_train_df = train_df.groupby(cols).apply(lambda df: df.set_index('target_name').target)\nagg_train_df.reset_index(inplace=True)\nagg_train_df.columns.name = None\n\nagg_train_df['image'] = agg_train_df.image_path.progress_apply(\n    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:34:44.379134Z","iopub.execute_input":"2026-01-18T13:34:44.379347Z","iopub.status.idle":"2026-01-18T13:35:00.911828Z","shell.execute_reply.started":"2026-01-18T13:34:44.379330Z","shell.execute_reply":"2026-01-18T13:35:00.910972Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_47/2742864213.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  agg_train_df = train_df.groupby(cols).apply(lambda df: df.set_index('target_name').target)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/357 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c25c4fbe0f4ec8a19f185ba84f2ad6"}},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"# 画像サイズ確認\nagg_train_df['image_size'] = agg_train_df.image.apply(lambda x: x.size)\nagg_train_df['image_size'].value_counts()\n\n# ターゲット合計確認\nnp.isclose(agg_train_df[['Dry_Green_g', 'Dry_Clover_g']].sum(axis=1),\n           agg_train_df['GDM_g'], atol=1e-4).mean()\n\nnp.isclose(agg_train_df[['GDM_g', 'Dry_Dead_g']].sum(axis=1),\n           agg_train_df['Dry_Total_g'], atol=1e-4).mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:00.914056Z","iopub.execute_input":"2026-01-18T13:35:00.914297Z","iopub.status.idle":"2026-01-18T13:35:00.927710Z","shell.execute_reply.started":"2026-01-18T13:35:00.914277Z","shell.execute_reply":"2026-01-18T13:35:00.926939Z"}},"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"0.9971988795518207"},"metadata":{}}],"execution_count":65},{"cell_type":"code","source":"# test.csv\ntest_df = pd.read_csv(DATA_ROOT + 'test.csv')\ntest_df[['sample_id_prefix', 'sample_id_suffix']] = test_df.sample_id.str.split('__', expand=True)\n\n# 推論用 agg_test_df\nagg_test_df = test_df.drop_duplicates(subset='sample_id_prefix').copy()\n\nagg_test_df['image'] = agg_test_df.image_path.progress_apply(\n    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:00.928641Z","iopub.execute_input":"2026-01-18T13:35:00.928977Z","iopub.status.idle":"2026-01-18T13:35:01.006171Z","shell.execute_reply.started":"2026-01-18T13:35:00.928948Z","shell.execute_reply":"2026-01-18T13:35:01.005321Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dee3e59834b4c50bbbbdb5cdaef4ace"}},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"class InferenceDataset(Dataset):\n    def __init__(self, df, transforms):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = self.df.iloc[idx][\"image\"]\n        width, height = image.size\n        mid_point = width // 2\n\n        # 左右に分割\n        left_image = image.crop((0, 0, mid_point, height))\n        right_image = image.crop((mid_point, 0, width, height))\n\n        if self.transforms:\n            left_image = self.transforms(image=np.array(left_image))[\"image\"]\n            right_image = self.transforms(image=np.array(right_image))[\"image\"]\n\n        return left_image, right_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.006777Z","iopub.execute_input":"2026-01-18T13:35:01.006960Z","iopub.status.idle":"2026-01-18T13:35:01.013023Z","shell.execute_reply.started":"2026-01-18T13:35:01.006945Z","shell.execute_reply":"2026-01-18T13:35:01.012263Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"!ls /kaggle/input/model-dinov3-large/model.safetensors","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.013803Z","iopub.execute_input":"2026-01-18T13:35:01.013998Z","iopub.status.idle":"2026-01-18T13:35:01.321304Z","shell.execute_reply.started":"2026-01-18T13:35:01.013977Z","shell.execute_reply":"2026-01-18T13:35:01.320538Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/model-dinov3-large/model.safetensors\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nimport os \n\nclass LocalMambaBlock(nn.Module):\n    \"\"\"\n    Lightweight Mamba-style block (Gated CNN) from the reference notebook.\n    Efficiently mixes tokens with linear complexity.\n    \"\"\"\n    def __init__(self, dim, kernel_size=5, dropout=0.0):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        # Depthwise conv mixes spatial information locally\n        self.dwconv = nn.Conv1d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n        self.gate = nn.Linear(dim, dim)\n        self.proj = nn.Linear(dim, dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # x: (Batch, Tokens, Dim)\n        shortcut = x\n        x = self.norm(x)\n        # Gating mechanism\n        g = torch.sigmoid(self.gate(x))\n        x = x * g\n        # Spatial mixing via 1D Conv (requires transpose)\n        x = x.transpose(1, 2)  # -> (B, D, N)\n        x = self.dwconv(x)\n        x = x.transpose(1, 2)  # -> (B, N, D)\n        # Projection\n        x = self.proj(x)\n        x = self.drop(x)\n        return shortcut + x\n\nclass TimmEncoder(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n\n        pretrained = True if cfg.model.resume_path is None else False\n\n        # 1. Load Backbone with global_pool='' to keep patch tokens\n        #    (B, 197, 1024) instead of (B, 1024)\n        self.encoder = timm.create_model(\n            cfg.model.backbone,\n            in_chans=cfg.task.slice_depth,\n            pretrained=False,\n            # drop_path_rate=cfg.model.drop_path_rate,\n            features_only=False,\n            num_classes=0,\n            global_pool=\"\",  # 自前でpoolingするのでここは空\n        )\n\n        # 2. Enable Gradient Checkpointing (Crucial for ViT-Large memory!)\n        if hasattr(self.encoder, 'set_grad_checkpointing'):\n            self.encoder.set_grad_checkpointing(True)\n            print(\"✓ Gradient Checkpointing enabled (saves ~50% VRAM)\")\n        \n        nf = self.encoder.num_features\n        \n        # 3. Mamba Fusion Neck\n        #    Mixes the concatenated tokens [Left, Right]\n        self.fusion = nn.Sequential(\n            LocalMambaBlock(nf, kernel_size=5, dropout=0.1),\n            LocalMambaBlock(nf, kernel_size=5, dropout=0.1)\n        )\n        \n        # 4. Pooling & Heads\n        self.pool = nn.AdaptiveAvgPool1d(1)\n        \n        # Heads (using the same logic as before, but on fused features)\n        self.head_green_raw  = nn.Sequential(\n            nn.Linear(nf, nf//2), nn.GELU(), nn.Dropout(0.2), \n            nn.Linear(nf//2, 1), nn.Softplus()\n        )\n        self.head_clover_raw = nn.Sequential(\n            nn.Linear(nf, nf//2), nn.GELU(), nn.Dropout(0.2), \n            nn.Linear(nf//2, 1), nn.Softplus()\n        )\n        self.head_dead_raw   = nn.Sequential(\n            nn.Linear(nf, nf//2), nn.GELU(), nn.Dropout(0.2), \n            nn.Linear(nf//2, 1), nn.Softplus()\n        )\n        \n        \n        if pretrained:\n            self.load_pretrained()\n    \n        if cfg.model.freeze_backbone:\n            for p in self.encoder.parameters():\n                p.requires_grad = False\n\n    def load_pretrained(self):\n        try:\n            path = self.cfg.model.backbone_path\n            if path and os.path.exists(path):\n                print(f\"Loading backbone weights from local file: {path}\")\n                \n                if path.endswith(\".safetensors\"):\n                    from safetensors.torch import load_file\n                    sd = load_file(path) # safetensors専用のロード\n                else:\n                    sd = torch.load(path, map_location='cpu')\n                \n                # wrapperの除去\n                if 'model' in sd: sd = sd['model']\n                elif 'state_dict' in sd: sd = sd['state_dict']\n                \n                # load_state_dictを実行\n                self.encoder.load_state_dict(sd, strict=False)\n                print('Successfully loaded local weights.')\n            else:\n                print(f\"Warning: backbone_path not found at {path}\")\n        except Exception as e:\n            print(f'Warning: pretrained load failed: {e}')\n\n    def forward(self, left_img: torch.Tensor, right_img: torch.Tensor):\n        # 1. Extract Tokens (B, N, D)\n        #    Note: ViT usually returns [CLS, Patch1, Patch2...]\n        #    We remove CLS token for spatial mixing, or keep it. Let's keep it.\n        x_l = self.encoder(left_img)\n        x_r = self.encoder(right_img)\n        # x_l = self.encoder.forward_features(left_img)\n        # x_r = self.encoder.forward_features(right_img)\n\n        # 2. Concatenate Left and Right tokens along sequence dimension\n        #    (B, N, D) + (B, N, D) -> (B, 2N, D)\n        x_cat = torch.cat([x_l, x_r], dim=1)\n        \n        # 3. Apply Mamba Fusion\n        #    This allows tokens from Left image to interact with tokens from Right image\n        x_fused = self.fusion(x_cat)\n        \n        # 4. Global Pooling\n        #    (B, 2N, D) -> (B, D, 2N) -> (B, D, 1) -> (B, D)\n        x_pool = self.pool(x_fused.transpose(1, 2)).flatten(1)\n        \n        # 5. Prediction Heads\n        green  = self.head_green_raw(x_pool)\n        clover = self.head_clover_raw(x_pool)\n        dead   = self.head_dead_raw(x_pool)\n        \n        # Summation logic\n        gdm    = green + clover\n        total  = gdm + dead\n        \n        return total, gdm, green, clover, dead\n\n    def set_grad_checkpointing(self, enable: bool = True):\n        self.encoder.set_grad_checkpointing(enable)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.322768Z","iopub.execute_input":"2026-01-18T13:35:01.323403Z","iopub.status.idle":"2026-01-18T13:35:01.341254Z","shell.execute_reply.started":"2026-01-18T13:35:01.323375Z","shell.execute_reply":"2026-01-18T13:35:01.340309Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"def get_model_from_cfg(cfg):\n    if cfg.model.arch == \"timm_encoder\":\n        model = TimmEncoder(cfg)\n    else:\n        raise ValueError(f\"Unknown model architecture: {cfg.model.arch}\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.342250Z","iopub.execute_input":"2026-01-18T13:35:01.342623Z","iopub.status.idle":"2026-01-18T13:35:01.359315Z","shell.execute_reply.started":"2026-01-18T13:35:01.342594Z","shell.execute_reply":"2026-01-18T13:35:01.358543Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"\ndef get_loss(cfg):\n    return MyLoss(cfg)\n\nclass MyLoss(nn.Module):\n    def __init__(self, cfg):\n        super(MyLoss, self).__init__()\n        self.cfg = cfg\n\n        # 基本は SmoothL1（元コードと同じ）\n        self.criterion = nn.SmoothL1Loss(beta=5.0, reduction=\"mean\")\n\n        # 将来の拡張用（今は使わないが cfg で制御できる）\n        self.use_weights = getattr(cfg.loss, \"use_weights\", False)\n        self.weights = getattr(cfg.loss, \"weights\", None)\n\n    def forward(self, y_pred, y_true):\n        \"\"\"\n        Args:\n            y_pred (tuple of Tensor): (total, gdm, green, clover, dead)\n                各 Tensor の形状 = (batch,)\n            y_true (Tensor): (batch, 5)\n                各列 = [Green, Dead, Clover, GDM, Total]\n        Returns:\n            dict:\n                {\n                    \"loss\": total_loss,\n                    \"loss_green\": ...,\n                    \"loss_dead\": ...,\n                    \"loss_clover\": ...,\n                    \"loss_gdm\": ...,\n                    \"loss_total\": ...,\n                }\n        \"\"\"\n        return_dict = {}\n        total, gdm, green, clover, dead = y_pred\n\n        # 個別損失を計算\n        l_green  = self.criterion(green.squeeze(),  y_true[:,0])\n        l_dead   = self.criterion(dead.squeeze(),   y_true[:,1])\n        l_clover = self.criterion(clover.squeeze(), y_true[:,2])\n        l_gdm    = self.criterion(gdm.squeeze(),    y_true[:,3])\n        l_total  = self.criterion(total.squeeze(),  y_true[:,4])\n\n        # 辞書に格納\n        return_dict[\"loss_green\"]  = l_green\n        return_dict[\"loss_dead\"]   = l_dead\n        return_dict[\"loss_clover\"] = l_clover\n        return_dict[\"loss_gdm\"]    = l_gdm\n        return_dict[\"loss_total\"]  = l_total\n\n        # 損失をまとめる\n        losses = torch.stack([l_green, l_dead, l_clover, l_gdm, l_total])\n\n        if self.use_weights and self.weights is not None:\n            w = torch.as_tensor(self.weights, device=losses.device, dtype=losses.dtype)\n            w = w / w.sum()\n            total_loss = (losses * w).sum()\n        else:\n            total_loss = losses.mean()\n\n        return_dict[\"loss\"] = total_loss\n        return return_dict\n\n\ndef main():\n    pass\n\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.362203Z","iopub.execute_input":"2026-01-18T13:35:01.362495Z","iopub.status.idle":"2026-01-18T13:35:01.374010Z","shell.execute_reply.started":"2026-01-18T13:35:01.362452Z","shell.execute_reply":"2026-01-18T13:35:01.373133Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nfrom pytorch_lightning.core.module import LightningModule\nfrom timm.utils import ModelEmaV2\nfrom timm.optim import create_optimizer_v2\nfrom timm.scheduler import create_scheduler_v2\nimport torch\n\nfrom timm.utils import ModelEmaV3\n\nclass MyModel(LightningModule):\n    def __init__(self, cfg, mode=\"train\"):\n        super().__init__()\n        self.preds = None\n        self.gts = None\n\n        self.cfg = cfg\n        self.mode = mode\n        \n        self.model = get_model_from_cfg(cfg)\n\n        # epoch 集計用\n        self.val_outputs = []\n        self.val_targets = []\n\n        if mode != \"test\" and cfg.model.ema:\n            self.model_ema = ModelEmaV3(\n                self.model,\n                decay=cfg.model.ema_decay,\n                update_after_step=cfg.model.ema_update_after_step,\n            )\n\n        self.loss = get_loss(cfg)\n\n\n    def forward(self, left_img, right_img):\n        return self.model(left_img, right_img)\n\n    def training_step(self, batch, batch_idx):\n        left_img, right_img, targets = batch  # (B, 5)\n        targets = targets.float()\n\n        # outputs = (total, gdm, green, clover, dead)\n        outputs = self(left_img, right_img)\n        loss_dict = self.loss(outputs, targets)\n\n        self.log_dict(\n            loss_dict,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            sync_dist=True,\n        )\n        return loss_dict[\"loss\"]\n\n    def on_train_batch_end(self, out, batch, batch_idx):\n        if self.cfg.model.ema:\n            self.model_ema.update(self.model)\n\n    def validation_step(self, batch, batch_idx):\n        left_img, right_img, targets = batch\n        targets = targets.float()\n\n        outputs = self(left_img, right_img)\n        loss_dict = self.loss(outputs, targets)\n\n        self.log(\"val_loss\", loss_dict[\"loss\"], prog_bar=True, sync_dist=True)\n        \n        self.val_outputs.append(tuple(o.detach() for o in outputs))\n        self.val_targets.append(targets.detach())\n\n        return loss_dict\n\n    def on_validation_epoch_end(self):\n        outputs = torch.cat(\n            [torch.stack(t, dim=1) for t in self.val_outputs],\n            dim=0\n        ).cpu().numpy()\n        outputs = outputs.squeeze(-1)  # (N, 5)\n\n        targets = torch.cat(self.val_targets).cpu().numpy()\n\n        weighted_r2, r2_scores = calc_metric(self.cfg, outputs, targets)\n\n        # メトリクスをログ\n        self.log(\"val_weighted_r2\", weighted_r2, prog_bar=True)\n\n        # 複数ターゲットなら個別ログも可\n        for i, r2 in enumerate(r2_scores):\n            self.log(f\"val_r2_target_{i}\", r2)\n\n        # 次epochに向けてクリア\n        self.val_outputs.clear()\n        self.val_targets.clear()\n\n    def configure_optimizers(self):\n        optimizer = create_optimizer_v2(model_or_params=self.model, **self.cfg.opt)\n\n        scheduler, _ = create_scheduler_v2(\n            optimizer=optimizer,\n            num_epochs=self.cfg.trainer.max_epochs,\n            **self.cfg.scheduler\n        )\n\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"interval\": \"epoch\",\n                \"monitor\": \"val_weighted_r2\",\n            },\n        }\n\n    def lr_scheduler_step(self, scheduler, metric):\n        scheduler.step(epoch=self.current_epoch)\n    #     # scheduler.step_update(num_updates=self.global_step)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.374905Z","iopub.execute_input":"2026-01-18T13:35:01.375128Z","iopub.status.idle":"2026-01-18T13:35:01.389536Z","shell.execute_reply.started":"2026-01-18T13:35:01.375112Z","shell.execute_reply":"2026-01-18T13:35:01.388773Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"!ls /kaggle/input/model-dinov3-large","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.390342Z","iopub.execute_input":"2026-01-18T13:35:01.390651Z","iopub.status.idle":"2026-01-18T13:35:01.682241Z","shell.execute_reply.started":"2026-01-18T13:35:01.390627Z","shell.execute_reply":"2026-01-18T13:35:01.681283Z"}},"outputs":[{"name":"stdout","text":"model.safetensors\n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"from types import SimpleNamespace\n\ncfg = SimpleNamespace()\n\n# --- task ---\ncfg.task = SimpleNamespace(\n    img_size=224,\n    img_depth=16,\n    fixed_depth=16,\n    slice_depth=3,\n    pretrain=False,\n    dirname=\"train_npzs\"\n)\n\n# --- model ---\ncfg.model = SimpleNamespace(\n    freeze_end_epoch=0,\n    arch=\"timm_encoder\",\n    in_channels=16,\n    out_channels=1,\n    depth=4,\n    base_filters=64,\n    dropout=0.1,\n    use_batchnorm=True,\n    activation=\"relu\",\n    swa=False,\n    freeze_backbone=False,\n    backbone=\"vit_large_patch16_dinov3_qkvb\",\n    backbone_path=\"/kaggle/input/model-dinov3-large/model.safetensors\",\n    ema=False,\n    resume_path=\"loaded\",\n    drop_path_rate=0.0,\n    img_size=128,\n    img_depth=16,\n    kernel_size=5,\n    class_num=5\n)\n\n# --- data ---\ncfg.data = SimpleNamespace(\n    fold_num=5,\n    fold_id=0,\n    num_workers=8,\n    batch_size=32,\n    train_all=False,\n    input_dir=None,\n    output_dir=None,\n    val_output_dir=None\n)\n\n# --- trainer ---\ncfg.trainer = SimpleNamespace(\n    max_epochs=30,\n    devices=\"auto\",\n    strategy=\"auto\",\n    check_val_every_n_epoch=5,\n    sync_batchnorm=False,\n    accelerator=\"gpu\",\n    precision=32,\n    gradient_clip_val=None,\n    accumulate_grad_batches=1,\n    deterministic=True\n)\n\n# --- test ---\ncfg.test = SimpleNamespace(\n    mode=\"test\",\n    output_dir=\"preds_results\"\n)\n\n# --- opt ---\ncfg.opt = SimpleNamespace(\n    opt=\"AdamW\",\n    lr=1e-3,\n    weight_decay=0.01\n)\n\n# --- scheduler ---\ncfg.scheduler = SimpleNamespace(\n    sched=\"cosine\",\n    min_lr=0.0,\n    warmup_epochs=0\n)\n\n# --- loss ---\ncfg.loss = SimpleNamespace(\n    mixup=0.0,\n    cutmix=0.0\n)\n\n# --- wandb ---\ncfg.wandb = SimpleNamespace(\n    project=\"csiro2025\",\n    name=\"exp_0\",\n    fast_dev_run=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.683304Z","iopub.execute_input":"2026-01-18T13:35:01.683591Z","iopub.status.idle":"2026-01-18T13:35:01.692458Z","shell.execute_reply.started":"2026-01-18T13:35:01.683555Z","shell.execute_reply":"2026-01-18T13:35:01.691726Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"!ls /kaggle/input/csiro-simple-exp10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.693078Z","iopub.execute_input":"2026-01-18T13:35:01.693267Z","iopub.status.idle":"2026-01-18T13:35:01.985983Z","shell.execute_reply.started":"2026-01-18T13:35:01.693251Z","shell.execute_reply":"2026-01-18T13:35:01.985097Z"}},"outputs":[{"name":"stdout","text":"exp_10_epoch079_val_loss9.1485.ckpt\n","output_type":"stream"}],"execution_count":75},{"cell_type":"code","source":"def get_val_transforms(cfg):\n    return A.Compose(\n        [\n            A.Resize(height=cfg.task.img_size, width=cfg.task.img_size, p=1),\n            # A.RandomScale(scale_limit=(1.0, 1.0), p=1),\n            # A.PadIfNeeded(min_height=cfg.task.img_size, min_width=cfg.task.img_size, p=1.0,\n            #              border_mode=cv2.BORDER_CONSTANT, value=0),\n            # A.Crop(y_max=self.cfg.data.val_img_h, x_max=self.cfg.data.val_img_w, p=1.0),\n            A.Normalize(p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n            ToTensorV2(p=1.0),\n        ],\n        p=1.0,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.987151Z","iopub.execute_input":"2026-01-18T13:35:01.987439Z","iopub.status.idle":"2026-01-18T13:35:01.993262Z","shell.execute_reply.started":"2026-01-18T13:35:01.987414Z","shell.execute_reply":"2026-01-18T13:35:01.992338Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nckpt_path = \"/kaggle/input/csiro-simple-exp10/exp_10_epoch079_val_loss9.1485.ckpt\"\n\nmodel = MyModel.load_from_checkpoint(ckpt_path, cfg=cfg, mode=\"test\")\nmodel.to(device)\nmodel.eval()\n\ntest_dataset = InferenceDataset(agg_test_df, get_val_transforms(cfg))\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n\ndef predict(model, dataloader, device):\n    model.to(device)\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for left_img, right_img in tqdm(dataloader): # tqdmを追加して進捗を表示\n            left_img = left_img.to(device)\n            right_img = right_img.to(device)\n            \n            # outputs = (total, gdm, green, clover, dead)\n            outputs = model(left_img, right_img)\n            \n            # submission.csv の作成ロジックに合わせて [green, clover, dead] を抽出\n            # 各ヘッドの出力は (batch, 1) なので concat して (batch, 3) にする\n            res = torch.cat([outputs[2], outputs[3], outputs[4]], dim=1)\n            preds.append(res.cpu())\n            \n    return torch.cat(preds).numpy()\n\npreds = predict(model, test_loader, device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:01.994061Z","iopub.execute_input":"2026-01-18T13:35:01.994329Z","iopub.status.idle":"2026-01-18T13:35:09.499456Z","shell.execute_reply.started":"2026-01-18T13:35:01.994305Z","shell.execute_reply":"2026-01-18T13:35:09.498679Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.6.0, which is newer than your current Lightning version: v2.5.5\n","output_type":"stream"},{"name":"stdout","text":"✓ Gradient Checkpointing enabled (saves ~50% VRAM)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60949829050947868508d85d526678d0"}},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"agg_test_df[['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']] = preds\nagg_test_df['GDM_g'] = agg_test_df.Dry_Green_g + agg_test_df.Dry_Clover_g\nagg_test_df['Dry_Total_g'] = agg_test_df.GDM_g + agg_test_df.Dry_Dead_g\n\ncols = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\nsub_df = agg_test_df.set_index('sample_id_prefix')[cols].stack().reset_index()\nsub_df.columns = ['sample_id_prefix', 'target_name', 'target']\nsub_df['sample_id'] = sub_df.sample_id_prefix + '__' + sub_df.target_name\n\nsub_df[['sample_id', 'target']].to_csv('submission.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:09.501297Z","iopub.execute_input":"2026-01-18T13:35:09.501714Z","iopub.status.idle":"2026-01-18T13:35:09.537554Z","shell.execute_reply.started":"2026-01-18T13:35:09.501679Z","shell.execute_reply":"2026-01-18T13:35:09.536818Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"sub_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:09.538517Z","iopub.execute_input":"2026-01-18T13:35:09.539225Z","iopub.status.idle":"2026-01-18T13:35:09.561434Z","shell.execute_reply.started":"2026-01-18T13:35:09.539196Z","shell.execute_reply":"2026-01-18T13:35:09.560786Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"  sample_id_prefix   target_name     target                   sample_id\n0     ID1001187975  Dry_Clover_g   2.835814  ID1001187975__Dry_Clover_g\n1     ID1001187975    Dry_Dead_g  14.239312    ID1001187975__Dry_Dead_g\n2     ID1001187975   Dry_Green_g  13.224589   ID1001187975__Dry_Green_g\n3     ID1001187975   Dry_Total_g  30.299717   ID1001187975__Dry_Total_g\n4     ID1001187975         GDM_g  16.060404         ID1001187975__GDM_g","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id_prefix</th>\n      <th>target_name</th>\n      <th>target</th>\n      <th>sample_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID1001187975</td>\n      <td>Dry_Clover_g</td>\n      <td>2.835814</td>\n      <td>ID1001187975__Dry_Clover_g</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID1001187975</td>\n      <td>Dry_Dead_g</td>\n      <td>14.239312</td>\n      <td>ID1001187975__Dry_Dead_g</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID1001187975</td>\n      <td>Dry_Green_g</td>\n      <td>13.224589</td>\n      <td>ID1001187975__Dry_Green_g</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID1001187975</td>\n      <td>Dry_Total_g</td>\n      <td>30.299717</td>\n      <td>ID1001187975__Dry_Total_g</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID1001187975</td>\n      <td>GDM_g</td>\n      <td>16.060404</td>\n      <td>ID1001187975__GDM_g</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"!head submission.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-18T13:35:09.562350Z","iopub.execute_input":"2026-01-18T13:35:09.562679Z","iopub.status.idle":"2026-01-18T13:35:09.879453Z","shell.execute_reply.started":"2026-01-18T13:35:09.562655Z","shell.execute_reply":"2026-01-18T13:35:09.878467Z"}},"outputs":[{"name":"stdout","text":"sample_id,target\nID1001187975__Dry_Clover_g,2.8358138\nID1001187975__Dry_Dead_g,14.239312\nID1001187975__Dry_Green_g,13.224589\nID1001187975__Dry_Total_g,30.299717\nID1001187975__GDM_g,16.060404\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}